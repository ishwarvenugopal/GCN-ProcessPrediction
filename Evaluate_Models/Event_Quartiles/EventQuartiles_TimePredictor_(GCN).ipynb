{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ishwarvenugopal/GCN-ProcessPrediction/blob/master/Evaluate_Models/Event_Quartiles/EventQuartiles_TimePredictor_(GCN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW0wmkGQYSNJ"
   },
   "source": [
    "# Importing necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "colab_type": "code",
    "id": "JoDejA1v8ju1",
    "outputId": "8ce20171-56e0-4ea2-9d06-651eef3fec7b"
   },
   "outputs": [],
   "source": [
    "# !pip install pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9rHPnPB6TAe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "#from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_algorithm\n",
    "from pm4py.objects.conversion.dfg import converter as dfg_conv\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.visualization.dfg import visualizer as dfg_vis_fact\n",
    "from pm4py.visualization.petrinet import visualizer as pn_vis\n",
    "\n",
    "# from torch_geometric.nn.inits import glorot, zeros \n",
    "\n",
    "#Unable to import above line, so manually copy-pasting the source code\n",
    "\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "import bisect\n",
    "import warnings\n",
    "from torch._utils import _accumulate\n",
    "from torch import randperm, default_generator\n",
    "\n",
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "        \n",
    "def random_split(dataset, lengths, generator=default_generator):\n",
    "    r\"\"\"\n",
    "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "    Optionally fix the generator for reproducible results, e.g.:\n",
    "\n",
    "    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): Dataset to be split\n",
    "        lengths (sequence): lengths of splits to be produced\n",
    "        generator (Generator): Generator used for the random permutation.\n",
    "    \"\"\"\n",
    "    if sum(lengths) != len(dataset):\n",
    "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
    "\n",
    "    indices = randperm(sum(lengths), generator=generator).tolist()\n",
    "    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rkd6Re9YYcX"
   },
   "source": [
    "# Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wbo3vcPS6ald"
   },
   "outputs": [],
   "source": [
    "# Helpdesk dataset\n",
    "\n",
    "path = '../../Data/helpdesk.csv'\n",
    "dataset = 'helpdesk'\n",
    "save_folder = '../../python_files/Results/helpdesk'\n",
    "num_nodes = 9\n",
    "\n",
    "# # BPI dataset\n",
    "\n",
    "# path = '../../Data/bpi_12_w.csv'\n",
    "# dataset = 'bpi'\n",
    "# save_folder = '../../python_files/Results/bpi'\n",
    "# num_nodes = 6 \n",
    "\n",
    "num_features = 4\n",
    "showProcessGraph = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cuda'\n",
    "num_epochs = 100\n",
    "seed_value = 42\n",
    "weighted_adjacency = False\n",
    "binary_adjacency = True\n",
    "laplacian_matrix = False\n",
    "variant = 'binary'\n",
    "num_runs = 4\n",
    "\n",
    "lr_optimum = 1e-03\n",
    "# lr_optimum = None  #if evaluation is for all three learning rates\n",
    "\n",
    "if lr_optimum == 1e-03:\n",
    "    lr_init = 0\n",
    "    lr_run_end = lr_init + 1\n",
    "elif lr_optimum == 1e-04:\n",
    "    lr_init = 1\n",
    "    lr_run_end = lr_init + 1\n",
    "elif lr_optimum == 1e-05:\n",
    "    lr_init = 2\n",
    "    lr_run_end = lr_init + 1\n",
    "elif lr_optimum == None:\n",
    "    lr_init = 0\n",
    "    lr_run_end = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlHB6xqR8VOl"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4jbzyGgjJAw"
   },
   "outputs": [],
   "source": [
    "def generate_features_with_EventQuartiles (df,total_activities,num_features):\n",
    "  lastcase = ''\n",
    "  firstLine = True\n",
    "  numlines = 0\n",
    "  casestarttime = None\n",
    "  lasteventtime = None\n",
    "  features = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        numlines+=1\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "    feature_list = [timediff,timediff2,timediff3,timediff4]\n",
    "    features.append(feature_list)\n",
    "\n",
    "  df['Feature Vector'] = features\n",
    "  \n",
    "  firstLine = True\n",
    "  case_lengths = []\n",
    "  case_pos = []\n",
    "  start_pos = 0\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if not firstLine:\n",
    "      if (row[3][1]==0):\n",
    "        end_pos = i-1\n",
    "        for j in range(start_pos,end_pos):\n",
    "          case_lengths[j]= current_case_length\n",
    "        start_pos = i\n",
    "        current_case_length = 1\n",
    "        case_lengths.append(current_case_length)\n",
    "        case_pos.append(current_case_length)\n",
    "      else:\n",
    "        current_case_length+= 1\n",
    "        case_lengths.append(current_case_length)\n",
    "        case_pos.append(current_case_length)\n",
    "    else:\n",
    "      current_case_length = 1\n",
    "      case_lengths.append(current_case_length)\n",
    "      case_pos.append(current_case_length)\n",
    "      firstLine = False\n",
    "\n",
    "  end_pos = len(case_lengths) - 1\n",
    "  for j in range(start_pos,end_pos):\n",
    "      case_lengths[j]= current_case_length\n",
    "\n",
    "  df['Case_Length'] = case_lengths\n",
    "  df['Case_position'] = case_pos\n",
    "  quartiles = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    temp = float(row[5])/ float(row[4])\n",
    "    temp = temp*4.0\n",
    "    if temp<=1:\n",
    "      quartiles.append(1)\n",
    "    elif (temp>1) and (temp<=2):\n",
    "      quartiles.append(2)\n",
    "    elif (temp>2) and (temp<=3):\n",
    "      quartiles.append(3)\n",
    "    elif (temp>3) and (temp<=4):\n",
    "      quartiles.append(4)\n",
    "\n",
    "  df['Quartile'] = quartiles\n",
    "\n",
    "  firstLine = True\n",
    "  NN_features =[]\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if firstLine:\n",
    "      features = np.zeros((total_activities,num_features))\n",
    "      features[row[1] - 1] = row[3]\n",
    "      firstLine = False\n",
    "    else:\n",
    "      if (row[3][0] == 0):\n",
    "        features = np.zeros((total_activities,num_features))\n",
    "        features[row[1] - 1] = row[3]\n",
    "      else:\n",
    "        features = np.copy(prev_row_features)\n",
    "        features[row[1] - 1] = row[3]\n",
    "    prev_row_features = features\n",
    "    NN_features.append([row['Quartile'],features])  \n",
    "  \n",
    "  return NN_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7kIG6467QUA"
   },
   "outputs": [],
   "source": [
    "def generate_labels(df,total_activities):\n",
    "  next_activity = []\n",
    "  next_timestamp = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_activity.append(total_activities)\n",
    "      else:\n",
    "        next_activity.append(row[1]-1)\n",
    "  next_activity.append(total_activities)\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_timestamp.append(0)\n",
    "      else:\n",
    "        next_timestamp.append(row[3][0])\n",
    "  next_timestamp.append(0)\n",
    "\n",
    "  return next_activity,next_timestamp\n",
    "\n",
    "class EventLogData(Dataset):\n",
    "  def __init__ (self, input, output):\n",
    "    self.X = input\n",
    "    self.y = output\n",
    "    self.y = self.y.to(torch.float32)\n",
    "    self.y = self.y.reshape((len(self.y),1))\n",
    "\n",
    "  #get the number of rows in the dataset\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  #get a row at a particular index in the dataset\n",
    "  def __getitem__ (self,idx):\n",
    "    return [self.X[idx],self.y[idx]]\n",
    "  \n",
    "  # get the indices for the train and test rows\n",
    "  def get_splits(self, n_test = 0.33, n_valid = 0.2):\n",
    "    train_idx,test_idx = train_test_split(list(range(len(self.X))),test_size = n_test, shuffle = False )\n",
    "    train_idx, valid_idx = train_test_split(train_idx, test_size = n_valid, shuffle = True)\n",
    "    train = Subset(self, train_idx)\n",
    "    valid = Subset(self, valid_idx)\n",
    "    test = Subset(self, test_idx)\n",
    "    return train, valid, test\n",
    "\n",
    "def prepare_data_for_Predictor(NN_features,label):\n",
    "  dataset = EventLogData(NN_features,label)\n",
    "  train, valid, test = dataset.get_splits()\n",
    "  train_dl = DataLoader(train, batch_size=1, shuffle = True)\n",
    "  valid_dl = DataLoader(valid, batch_size=1, shuffle = False)\n",
    "  test_dl = DataLoader(test, batch_size = 1, shuffle = False)\n",
    "  return train_dl, valid_dl, test_dl\n",
    "\n",
    "def generate_input_and_labels (path):\n",
    "  df = pd.read_csv(path)\n",
    "  total_unique_activities = num_nodes\n",
    "  NN_features = generate_features_with_EventQuartiles(df,total_unique_activities,num_features)\n",
    "  next_activity, next_timestamp = generate_labels(df,total_unique_activities)\n",
    "  # NN_features = torch.Tensor(NN_features).to(torch.float32)\n",
    "  next_activity = torch.Tensor(next_activity).to(torch.float32)\n",
    "  next_timestamp = torch.Tensor(next_timestamp).to(torch.float32)\n",
    " \n",
    "  train_dl, valid_dl, test_dl = prepare_data_for_Predictor(NN_features, next_timestamp)\n",
    "  \n",
    "  return train_dl,valid_dl,test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Upqo-IbWYiCG"
   },
   "source": [
    "# Getting Adjacency Matrix from Process Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-cisiZP9_i3"
   },
   "outputs": [],
   "source": [
    "def generate_process_graph (path):\n",
    "  data = pd.read_csv(path)\n",
    "  num_nodes = data['ActivityID'].nunique() # 9 for helpdesk.csv\n",
    "  cols = ['case:concept:name','concept:name','time:timestamp']\n",
    "  data.columns = cols \n",
    "  data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "  data['concept:name'] = data['concept:name'].astype(str)\n",
    "  log = log_converter.apply(data, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "  dfg = dfg_algorithm.apply(log)\n",
    "  if showProcessGraph:\n",
    "    visualize_process_graph(dfg,log)\n",
    "  max = 0\n",
    "  min = 0\n",
    "  adj = np.zeros((num_nodes,num_nodes))\n",
    "  for k,v in dfg.items():\n",
    "    for i in range(num_nodes):\n",
    "      if(k[0] == str(i+1)):\n",
    "        for j in range(num_nodes):\n",
    "          if (k[1] == str(j+1)):\n",
    "            adj[i][j] = v\n",
    "            if (v > max): max=v\n",
    "            if (v< min): min=v\n",
    "\n",
    "  # print(\"Raw weighted adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  if binary_adjacency:\n",
    "    for i in range(num_nodes):\n",
    "      for j in range(num_nodes):\n",
    "        if (adj[i][j]!=0):\n",
    "          adj[i][j]=1\n",
    "    # print(\"Binary adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  D = np.array(np.sum(adj, axis=1))\n",
    "  D = np.matrix(np.diag(D))\n",
    "  # print(\"Degree matrix: {}\".format(adj))\n",
    "  \n",
    "  adj = np.matrix(adj)\n",
    "\n",
    "  if laplacian_matrix:\n",
    "    adj = D - adj # Laplacian Transform \n",
    "    # print(\"Laplacian matrix: {}\".format(adj))\n",
    "\n",
    "  # adj = (D**-1)*adj\n",
    "  adj = fractional_matrix_power(D, -0.5)*adj*fractional_matrix_power(D, -0.5)\n",
    "  adj = torch.Tensor(adj).to(torch.float)\n",
    "  \n",
    "  # print(\"Symmetrically normalised Adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  return adj\n",
    "\n",
    "def visualize_process_graph (dfg,log):\n",
    "  dfg_gv = dfg_vis_fact.apply(dfg, log, parameters={dfg_vis_fact.Variants.FREQUENCY.value.Parameters.FORMAT: \"jpeg\"})\n",
    "  dfg_vis_fact.view(dfg_gv)\n",
    "  dfg_vis_fact.save(dfg_gv,\"dfg.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztVjtvxqYnb3"
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW3XlEZ591GP"
   },
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_features, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.in_channels = num_features\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(num_features, out_channels))\n",
    "        self.bias = Parameter(torch.Tensor(num_nodes))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = adj@x@self.weight\n",
    "        x = torch.flatten(x)\n",
    "        x = x + self.bias\n",
    "        return x\n",
    "\n",
    "class TimePredictor(torch.nn.Module):\n",
    "    def __init__(self,num_nodes, num_features = 4):\n",
    "        super(TimePredictor, self).__init__()\n",
    "\n",
    "        self.layer1 = GCNConv(num_nodes , num_features, out_channels=1)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(num_nodes,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256,1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.layer1(x,adj)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lb_PZMgF3qoo"
   },
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIjMtIDLGVpb"
   },
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "  predictions, actuals = list(),list()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    testing_loss_1 = 0\n",
    "    testing_loss_2 = 0\n",
    "    testing_loss_3 = 0\n",
    "    testing_loss_4 = 0\n",
    "    testing_loss_all = 0\n",
    "    num_of_minibatch = 0\n",
    "    num_1 = 0\n",
    "    num_2 = 0\n",
    "    num_3 = 0\n",
    "    num_4 = 0\n",
    "  \n",
    "    for i,(inputs,targets) in enumerate(test_dl):\n",
    "    \n",
    "      input_quartile = int(inputs[0][0])\n",
    "      inputs = inputs[1].to(torch.float32)\n",
    "      \n",
    "      inputs,targets = inputs.to(device),targets.to(device)\n",
    "\n",
    "      yhat = model(inputs[0],adj)\n",
    "    \n",
    "      loss = criterion(yhat.reshape((1,-1)),targets[0].to(torch.long).reshape((1,-1)))\n",
    "      testing_loss_all+= loss.item()\n",
    "      num_of_minibatch+= 1\n",
    "\n",
    "      if (input_quartile==1):\n",
    "        testing_loss_1+= loss.item()\n",
    "        num_1+=1\n",
    "      elif (input_quartile==2):\n",
    "        testing_loss_2+= loss.item()\n",
    "        num_2+=1\n",
    "      elif (input_quartile==3):\n",
    "        testing_loss_3+= loss.item()\n",
    "        num_3+=1\n",
    "      elif (input_quartile==4):\n",
    "        testing_loss_4+= loss.item()\n",
    "        num_4+=1\n",
    "\n",
    "  avg_loss_1 = testing_loss_1/num_1\n",
    "  avg_loss_1 = avg_loss_1/86400\n",
    "  avg_loss_2 = testing_loss_2/num_2\n",
    "  avg_loss_2 = avg_loss_2/86400\n",
    "  avg_loss_3 = testing_loss_3/num_3\n",
    "  avg_loss_3 = avg_loss_3/86400\n",
    "  avg_loss_4 = testing_loss_4/num_4\n",
    "  avg_loss_4 = avg_loss_4/86400\n",
    "  avg_loss_all = testing_loss_all/num_of_minibatch\n",
    "  avg_loss_all = avg_loss_all/86400\n",
    "  return avg_loss_1,avg_loss_2,avg_loss_3,avg_loss_4,avg_loss_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1ppaZVGm7Fk"
   },
   "outputs": [],
   "source": [
    "ma_errors = {}\n",
    "lr_run = lr_init\n",
    "for lr_run in range(lr_run, lr_run_end):\n",
    "  if lr_run==0:\n",
    "    lr_value = 1e-03\n",
    "  elif lr_run==1:\n",
    "    lr_value = 1e-04\n",
    "  elif lr_run==2:\n",
    "    lr_value = 1e-05\n",
    "  model = TimePredictor(num_nodes, num_features)  \n",
    "  train_dl, valid_dl, test_dl = generate_input_and_labels(path)\n",
    "  adj = generate_process_graph(path)\n",
    "  criterion = nn.L1Loss()\n",
    "  # optimizer = torch.optim.Adam(model.parameters(),lr=lr_value)\n",
    "\n",
    "  # print(\"************* Timestamp Predictor ***************\")\n",
    "  # print(\"Train size: {}, Validation size:{}, Test size: {}\".format(len(train_dl.dataset),len(valid_dl.dataset),len(test_dl.dataset)))\n",
    "  # print(model)\n",
    "  model = model.to(device)\n",
    "  adj = adj.to(device)\n",
    "  run = 0\n",
    "\n",
    "  quartile_1 = []\n",
    "  quartile_2 = []\n",
    "  quartile_3 = []\n",
    "  quartile_4 = []\n",
    "  overall = []\n",
    "\n",
    "  for run in range(num_runs):\n",
    "    model.load_state_dict(torch.load('{}/TimestampPredictor_parameters_{}_{}_{}_run{}.pt'.format(save_folder,dataset,variant,lr_value,run),map_location=torch.device(device)))\n",
    "    avg_loss_1,avg_loss_2,avg_loss_3,avg_loss_4,avg_loss_all = evaluate_model()\n",
    "    # print(\"MAE for Quartile 1: {}\".format(avg_loss_1))\n",
    "    # print(\"MAE for Quartile 2: {}\".format(avg_loss_2))\n",
    "    # print(\"MAE for Quartile 3: {}\".format(avg_loss_3))\n",
    "    # print(\"MAE for Quartile 4: {}\".format(avg_loss_4))\n",
    "    # print(\"Overall MAE: {}\".format(avg_loss_all))\n",
    "    quartile_1.append(avg_loss_1)\n",
    "    quartile_2.append(avg_loss_2)\n",
    "    quartile_3.append(avg_loss_3)\n",
    "    quartile_4.append(avg_loss_4)\n",
    "    overall.append(avg_loss_all)\n",
    "\n",
    "  compile = [quartile_1,quartile_2,quartile_3,quartile_4,overall]\n",
    "  ma_errors[lr_value] = compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYOQvzJknNIV"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset: {}\".format(dataset))\n",
    "print(\"Model Variant: {}\".format(variant))\n",
    "\n",
    "data = []\n",
    "lr_run = lr_init\n",
    "for lr_run in range(lr_run, lr_run_end):\n",
    "  if lr_run==0:\n",
    "    lr_value = 1e-03\n",
    "  elif lr_run==1:\n",
    "    lr_value = 1e-04\n",
    "  elif lr_run==2:\n",
    "    lr_value = 1e-05\n",
    "  q1_avg = np.round(np.mean(np.array(ma_errors[lr_value][0])),4)\n",
    "  q1_std = np.round(np.std(np.array(ma_errors[lr_value][0])),4)\n",
    "  q2_avg = np.round(np.mean(np.array(ma_errors[lr_value][1])),4) \n",
    "  q2_std = np.round(np.std(np.array(ma_errors[lr_value][1])),4)\n",
    "  q3_avg = np.round(np.mean(np.array(ma_errors[lr_value][2])),4) \n",
    "  q3_std = np.round(np.std(np.array(ma_errors[lr_value][2])),4)\n",
    "  q4_avg = np.round(np.mean(np.array(ma_errors[lr_value][3])),4) \n",
    "  q4_std = np.round(np.std(np.array(ma_errors[lr_value][3])),4)\n",
    "  qall_avg = np.round(np.mean(np.array(ma_errors[lr_value][4])),4) \n",
    "  qall_std = np.round(np.std(np.array(ma_errors[lr_value][4])),4)\n",
    "  \n",
    "  print(\"Learning rate: {}, Overall MAE: {}\".format(lr_value,qall_avg))\n",
    "  row = [variant, lr_value, '{} +/- {}'.format(q1_avg,q1_std),'{} +/- {}'.format(q2_avg,q2_std),'{} +/- {}'.format(q3_avg,q3_std),'{} +/- {}'.format(q4_avg,q4_std),'{} +/- {}'.format(qall_avg,qall_std)]  \n",
    "  data.append(row)\n",
    "\n",
    "columns = ['Variant','Learning Rate','Quartile_1','Quartile_2','Quartile_3','Quartile_4','Overall']\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "df.to_csv('EventQuartiles_MAE_{}_{}.csv'.format(dataset,variant),index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7813elGjxHFF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN+ia7axcYbNCacDDq/fbIr",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1YFaNfNRNtLqQb5jFYP-Hplqdel3VpHSp",
   "name": "Copy of Copy of EventQuartiles_TimePredictor_(GCN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
