{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ishwarvenugopal/GCN-ProcessPrediction/blob/master/GCN_TimePredictor_(Training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW0wmkGQYSNJ"
   },
   "source": [
    "# Importing necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JoDejA1v8ju1",
    "outputId": "5f0c6202-d4e1-4cca-b819-c0c86a4a9d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pm4py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/3d/28f5592abe66ebf47b3427bfa1f880a1d84400f88c5e78a1ad5de1abc410/pm4py-1.5.1-py3-none-any.whl (694kB)\n",
      "\r",
      "\u001b[K     |▌                               | 10kB 30.9MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 30kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 40kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 51kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 61kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 71kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 81kB 5.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 92kB 5.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 102kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 112kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 122kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 133kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 143kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 153kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 163kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 174kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 184kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 194kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 204kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 215kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 225kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 235kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 245kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 256kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 266kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 276kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 286kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 296kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 307kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 317kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 327kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 337kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 348kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 358kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 368kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 378kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 389kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 399kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 409kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 419kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 430kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 440kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 450kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 460kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 471kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 481kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 491kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 501kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 512kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 522kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 532kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 542kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 552kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 563kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 573kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 583kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 593kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 604kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 614kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 624kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 634kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 645kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 655kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 665kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 675kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 686kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 696kB 5.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pm4py) (1.18.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pm4py) (1.0.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pm4py) (0.22.2.post1)\n",
      "Collecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pm4py) (4.2.6)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from pm4py) (0.10.1)\n",
      "Requirement already satisfied: pydotplus in /usr/local/lib/python3.6/dist-packages (from pm4py) (2.0.2)\n",
      "Collecting stringdist\n",
      "  Downloading https://files.pythonhosted.org/packages/85/f0/c56cbe92b4b06fbc7adaa81917ad34d7027834e166fff2d2db73961c67fa/StringDist-1.0.9.tar.gz\n",
      "Collecting pm4pycvxopt>=0.0.9; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/15/d1/20b3b622c4c5a3df658e0719c585ef3b37a483653dfe95b8fc0e2424b6a9/pm4pycvxopt-0.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pm4py) (1.4.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from pm4py) (1.1.1)\n",
      "Requirement already satisfied: intervaltree in /usr/local/lib/python3.6/dist-packages (from pm4py) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pm4py) (4.41.1)\n",
      "Collecting ciso8601; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/da/626910cf8aca7ed2d5b34355eee8aeaaeb6ddd4e16f98d00a9e2ddad3a08/ciso8601-2.1.3.tar.gz\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from pm4py) (2018.9)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pm4py) (3.2.2)\n",
      "Collecting deprecation\n",
      "  Downloading https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
      "Collecting pyvis\n",
      "  Downloading https://files.pythonhosted.org/packages/77/e1/bee44bc4ec826bb7ed1f1ff52244bca3745b1a7d72033fc84ffeed61725b/pyvis-0.1.8.2-py3-none-any.whl\n",
      "Collecting pulp<=2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/34/757c88c320f80ce602199603afe63aed1e0bc11180b9a9fb6018fb2ce7ef/PuLP-2.1-py3-none-any.whl (40.6MB)\n",
      "\u001b[K     |████████████████████████████████| 40.6MB 69kB/s \n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pm4py) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pm4py) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pm4py) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->pm4py) (1.7.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from pydotplus->pm4py) (2.4.7)\n",
      "Requirement already satisfied: cvxopt in /usr/local/lib/python3.6/dist-packages (from pm4pycvxopt>=0.0.9; python_version < \"3.8\"->pm4py) (1.2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->pm4py) (1.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from intervaltree->pm4py) (2.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pm4py) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pm4py) (1.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from deprecation->pm4py) (20.4)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from pyvis->pm4py) (5.5.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.6/dist-packages (from pyvis->pm4py) (2.11.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pm4py) (4.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->pm4py) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->pm4py) (3.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (4.3.3)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (50.3.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (1.0.18)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (4.8.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (0.8.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.3.0->pyvis->pm4py) (0.7.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.9.6->pyvis->pm4py) (1.1.1)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.3.0->pyvis->pm4py) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis->pm4py) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.3.0->pyvis->pm4py) (0.6.0)\n",
      "Building wheels for collected packages: stringdist, ciso8601\n",
      "  Building wheel for stringdist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for stringdist: filename=StringDist-1.0.9-cp36-cp36m-linux_x86_64.whl size=24375 sha256=bfbeee9c3b0760ead0e87bb037b1309c733210ab2b91215c98f467033b2f3d84\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/1b/bb/bf0de4d64d8ca38759811fe3353e441f12feb606fb3d1d2e11\n",
      "  Building wheel for ciso8601 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ciso8601: filename=ciso8601-2.1.3-cp36-cp36m-linux_x86_64.whl size=28297 sha256=55a5b44e130cd3ce87ea7e83a4889b293c4361918ef781aaa27ef3b377525abb\n",
      "  Stored in directory: /root/.cache/pip/wheels/eb/32/e4/13bdaf7e245f82667b21e0cfb03d21224691a47fa9f9bc80a6\n",
      "Successfully built stringdist ciso8601\n",
      "Installing collected packages: jsonpickle, stringdist, pm4pycvxopt, ciso8601, deprecation, pyvis, pulp, pm4py\n",
      "Successfully installed ciso8601-2.1.3 deprecation-2.1.0 jsonpickle-1.4.1 pm4py-1.5.1 pm4pycvxopt-0.0.9 pulp-2.1 pyvis-0.1.8.2 stringdist-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9rHPnPB6TAe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "#from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_algorithm\n",
    "from pm4py.objects.conversion.dfg import converter as dfg_conv\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.visualization.dfg import visualizer as dfg_vis_fact\n",
    "from pm4py.visualization.petrinet import visualizer as pn_vis\n",
    "\n",
    "# from torch_geometric.nn.inits import glorot, zeros \n",
    "\n",
    "#Unable to import above line, so manually copy-pasting the source code\n",
    "\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "import bisect\n",
    "import warnings\n",
    "from torch._utils import _accumulate\n",
    "from torch import randperm, default_generator\n",
    "\n",
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "        \n",
    "def random_split(dataset, lengths, generator=default_generator):\n",
    "    r\"\"\"\n",
    "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "    Optionally fix the generator for reproducible results, e.g.:\n",
    "\n",
    "    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): Dataset to be split\n",
    "        lengths (sequence): lengths of splits to be produced\n",
    "        generator (Generator): Generator used for the random permutation.\n",
    "    \"\"\"\n",
    "    if sum(lengths) != len(dataset):\n",
    "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
    "\n",
    "    indices = randperm(sum(lengths), generator=generator).tolist()\n",
    "    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rkd6Re9YYcX"
   },
   "source": [
    "# Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wbo3vcPS6ald"
   },
   "outputs": [],
   "source": [
    "# Helpdesk dataset\n",
    "\n",
    "path = 'Data/helpdesk.csv'\n",
    "save_folder = 'Results/helpdesk'\n",
    "dataset = 'helpdesk'\n",
    "num_nodes = 9\n",
    "\n",
    "# # BPI dataset\n",
    "\n",
    "# path = 'Data/bpi_12_w.csv'\n",
    "# save_folder = 'Results/bpi'\n",
    "# dataset = 'bpi'\n",
    "# num_nodes = 6 \n",
    "\n",
    "num_features = 4\n",
    "showProcessGraph = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cuda'\n",
    "num_epochs = 100\n",
    "seed_value = 42\n",
    "# lr_value = 1e-05\n",
    "weighted_adjacency = False\n",
    "binary_adjacency = True\n",
    "laplacian_matrix = True\n",
    "variant = 'laplacianOnBinary' # Choose from ['weighted','binary','laplacianOnWeighted','laplacianOnBinary']\n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlHB6xqR8VOl"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6BZ98xW4fSY"
   },
   "outputs": [],
   "source": [
    "def generate_features (df,total_activities,num_features):\n",
    "  lastcase = ''\n",
    "  firstLine = True\n",
    "  numlines = 0\n",
    "  casestarttime = None\n",
    "  lasteventtime = None\n",
    "  features = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        numlines+=1\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "    feature_list = [timediff,timediff2,timediff3,timediff4]\n",
    "    features.append(feature_list)\n",
    "\n",
    "  df['Feature Vector'] = features\n",
    "  \n",
    "  firstLine = True\n",
    "  NN_features =[]\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if firstLine:\n",
    "      features = np.zeros((total_activities,num_features))\n",
    "      features[row[1] - 1] = row[3]\n",
    "      firstLine = False\n",
    "    else:\n",
    "      if (row[3][0] == 0):\n",
    "        features = np.zeros((total_activities,num_features))\n",
    "        features[row[1] - 1] = row[3]\n",
    "      else:\n",
    "        features = np.copy(prev_row_features)\n",
    "        features[row[1] - 1] = row[3]\n",
    "    prev_row_features = features\n",
    "    NN_features.append(features)  \n",
    "  \n",
    "  return NN_features\n",
    "\n",
    "def generate_labels(df,total_activities):\n",
    "  next_activity = []\n",
    "  next_timestamp = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_activity.append(total_activities)\n",
    "      else:\n",
    "        next_activity.append(row[1]-1)\n",
    "  next_activity.append(total_activities)\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_timestamp.append(0)\n",
    "      else:\n",
    "        next_timestamp.append(row[3][0])\n",
    "  next_timestamp.append(0)\n",
    "\n",
    "  return next_activity,next_timestamp\n",
    "\n",
    "class EventLogData(Dataset):\n",
    "  def __init__ (self, input, output):\n",
    "    self.X = input\n",
    "    self.y = output\n",
    "    self.y = self.y.to(torch.float32)\n",
    "    self.y = self.y.reshape((len(self.y),1))\n",
    "\n",
    "  #get the number of rows in the dataset\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  #get a row at a particular index in the dataset\n",
    "  def __getitem__ (self,idx):\n",
    "    return [self.X[idx],self.y[idx]]\n",
    "  \n",
    "  # get the indices for the train and test rows\n",
    "  def get_splits(self, n_test = 0.33, n_valid = 0.2):\n",
    "    train_idx,test_idx = train_test_split(list(range(len(self.X))),test_size = n_test, shuffle = False )\n",
    "    train_idx, valid_idx = train_test_split(train_idx, test_size = n_valid, shuffle = True)\n",
    "    train = Subset(self, train_idx)\n",
    "    valid = Subset(self, valid_idx)\n",
    "    test = Subset(self, test_idx)\n",
    "    return train, valid, test\n",
    "\n",
    "def prepare_data_for_Predictor(NN_features,label):\n",
    "  dataset = EventLogData(NN_features,label)\n",
    "  train, valid, test = dataset.get_splits()\n",
    "  train_dl = DataLoader(train, batch_size=1, shuffle = True)\n",
    "  valid_dl = DataLoader(valid, batch_size=1, shuffle = False)\n",
    "  test_dl = DataLoader(test, batch_size = 1, shuffle = False)\n",
    "  return train_dl, valid_dl, test_dl\n",
    "\n",
    "def generate_input_and_labels (path):\n",
    "  df = pd.read_csv(path)\n",
    "  total_unique_activities = num_nodes\n",
    "  NN_features = generate_features(df,total_unique_activities,num_features)\n",
    "  next_activity, next_timestamp = generate_labels(df,total_unique_activities)\n",
    "  NN_features = torch.Tensor(NN_features).to(torch.float32)\n",
    "  next_activity = torch.Tensor(next_activity).to(torch.float32)\n",
    "  next_timestamp = torch.Tensor(next_timestamp).to(torch.float32)\n",
    " \n",
    "  train_dl, valid_dl, test_dl = prepare_data_for_Predictor(NN_features, next_timestamp)\n",
    "  \n",
    "  return train_dl,valid_dl,test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Upqo-IbWYiCG"
   },
   "source": [
    "# Getting Adjacency Matrix from Process Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-cisiZP9_i3"
   },
   "outputs": [],
   "source": [
    "def generate_process_graph (path):\n",
    "  data = pd.read_csv(path)\n",
    "  num_nodes = data['ActivityID'].nunique() # 9 for helpdesk.csv\n",
    "  cols = ['case:concept:name','concept:name','time:timestamp']\n",
    "  data.columns = cols \n",
    "  data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "  data['concept:name'] = data['concept:name'].astype(str)\n",
    "  log = log_converter.apply(data, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "  dfg = dfg_algorithm.apply(log)\n",
    "  if showProcessGraph:\n",
    "    visualize_process_graph(dfg,log)\n",
    "  max = 0\n",
    "  min = 0\n",
    "  adj = np.zeros((num_nodes,num_nodes))\n",
    "  for k,v in dfg.items():\n",
    "    for i in range(num_nodes):\n",
    "      if(k[0] == str(i+1)):\n",
    "        for j in range(num_nodes):\n",
    "          if (k[1] == str(j+1)):\n",
    "            adj[i][j] = v\n",
    "            if (v > max): max=v\n",
    "            if (v< min): min=v\n",
    "\n",
    "  # print(\"Raw weighted adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  if binary_adjacency:\n",
    "    for i in range(num_nodes):\n",
    "      for j in range(num_nodes):\n",
    "        if (adj[i][j]!=0):\n",
    "          adj[i][j]=1\n",
    "    # print(\"Binary adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  D = np.array(np.sum(adj, axis=1))\n",
    "  D = np.matrix(np.diag(D))\n",
    "  # print(\"Degree matrix: {}\".format(D))\n",
    "  \n",
    "  adj = np.matrix(adj)\n",
    "\n",
    "  if laplacian_matrix:\n",
    "    adj = D - adj # Laplacian Transform \n",
    "    # print(\"Laplacian matrix: {}\".format(adj))\n",
    "\n",
    "  # adj = (D**-1)*adj\n",
    "  adj = fractional_matrix_power(D, -0.5)*adj*fractional_matrix_power(D, -0.5)\n",
    "  adj = torch.Tensor(adj).to(torch.float)\n",
    "  \n",
    "  # print(\"Symmetrically normalised Adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  return adj\n",
    "\n",
    "def visualize_process_graph (dfg,log):\n",
    "  dfg_gv = dfg_vis_fact.apply(dfg, log, parameters={dfg_vis_fact.Variants.FREQUENCY.value.Parameters.FORMAT: \"jpeg\"})\n",
    "  dfg_vis_fact.view(dfg_gv)\n",
    "  dfg_vis_fact.save(dfg_gv,\"dfg.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztVjtvxqYnb3"
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW3XlEZ591GP"
   },
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_features, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.in_channels = num_features\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(num_features, out_channels))\n",
    "        self.bias = Parameter(torch.Tensor(num_nodes))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = adj@x@self.weight\n",
    "        x = torch.flatten(x)\n",
    "        x = x + self.bias\n",
    "        return x\n",
    "\n",
    "class TimePredictor(torch.nn.Module):\n",
    "    def __init__(self,num_nodes, num_features = 4):\n",
    "        super(TimePredictor, self).__init__()\n",
    "\n",
    "        self.layer1 = GCNConv(num_nodes , num_features, out_channels=1)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(num_nodes,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256,1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.layer1(x,adj)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdGDSmmyYtg_"
   },
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIatIuLiFnwK"
   },
   "outputs": [],
   "source": [
    "lr_run = 0\n",
    "for lr_run in range(3):\n",
    "  if lr_run==0:\n",
    "    lr_value = 1e-03\n",
    "  elif lr_run==1:\n",
    "    lr_value = 1e-04\n",
    "  elif lr_run==2:\n",
    "    lr_value = 1e-05\n",
    "  run = 0\n",
    "  for run in range(num_runs):\n",
    "    print(\"Run: {}, Learning Rate: {}\".format(run+1,lr_value))\n",
    "    model = TimePredictor(num_nodes, num_features)  \n",
    "    train_dl, valid_dl, test_dl = generate_input_and_labels(path)\n",
    "    adj = generate_process_graph(path)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr_value)\n",
    "\n",
    "    # print(\"************* Timestamp Predictor ***************\")\n",
    "    # print(\"Train size: {}, Validation size:{}, Test size: {}\".format(len(train_dl.dataset),len(valid_dl.dataset),len(test_dl.dataset)))\n",
    "    # print(model)\n",
    "    model = model.to(device)\n",
    "    adj = adj.to(device)\n",
    "    epochs_plt = []\n",
    "    mae_plt = []\n",
    "    valid_loss_plt = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        predictions, actuals = list(),list()\n",
    "        num_train = 0\n",
    "\n",
    "        for i, (inputs,targets) in enumerate(train_dl):\n",
    "\n",
    "          inputs,targets = inputs.to(device), targets.to(device)\n",
    "          optimizer.zero_grad() # Clearing the gradients\n",
    "\n",
    "          yhat = model(inputs[0],adj)\n",
    "          loss = criterion(yhat.reshape((1,-1)),targets[0].to(torch.long).reshape((1,-1)))\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          training_loss+= loss.item()\n",
    "          num_train+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "          model.eval()\n",
    "          num_valid = 0\n",
    "          validation_loss = 0\n",
    "          for i,(inputs,targets) in enumerate(valid_dl):\n",
    "            inputs,targets = inputs.to(device),targets.to(device)\n",
    "            yhat_valid = model(inputs[0],adj)\n",
    "            loss_valid = criterion(yhat_valid.reshape((1,-1)),targets[0].to(torch.long).reshape((1,-1)))\n",
    "            validation_loss+= loss_valid.item()\n",
    "            num_valid+= 1\n",
    "\n",
    "        avg_training_loss = training_loss/num_train\n",
    "        avg_training_loss = avg_training_loss/86400\n",
    "        avg_validation_loss = validation_loss/num_valid\n",
    "        avg_validation_loss = avg_validation_loss/86400\n",
    "\n",
    "        if (epoch==0): \n",
    "          best_loss = avg_validation_loss\n",
    "          torch.save(model.state_dict(),'TimestampPredictor_parameters.pt')\n",
    "        \n",
    "        if (avg_validation_loss < best_loss):\n",
    "          torch.save(model.state_dict(),'TimestampPredictor_parameters.pt')\n",
    "          best_loss = avg_validation_loss\n",
    "          \n",
    "        print(\"Epoch: {}, Training MAE : {}, Validation loss : {}\".format(epoch,avg_training_loss,avg_validation_loss))\n",
    "        epochs_plt.append(epoch+1)\n",
    "        mae_plt.append(avg_training_loss)\n",
    "        valid_loss_plt.append(avg_validation_loss)\n",
    "\n",
    "    model.load_state_dict(torch.load('TimestampPredictor_parameters.pt'))\n",
    "    torch.save(model.state_dict(),'{}/TimestampPredictor_parameters_{}_{}_{}_run{}.pt'.format(save_folder,dataset,variant,lr_value,run))\n",
    "    filepath = '{}/Loss_{}_{}_{}_run{}.txt'.format(save_folder,dataset,variant,lr_value,run)\n",
    "\n",
    "    with open(filepath, 'w') as file:\n",
    "        for item in zip(epochs_plt,mae_plt,valid_loss_plt):\n",
    "            file.write(\"{}\\n\".format(item))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOPS/LnFBo+T70fVUPztuEf",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1dGjxvgWOx7LkyMw3aJTimuqvfY0xLWGn",
   "name": "Copy of GCN_TimePredictor_(Training).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
