{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ishwarvenugopal/GCN-ProcessPrediction/blob/master/GCN_TimePredictor_(Training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW0wmkGQYSNJ"
   },
   "source": [
    "# Importing necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JoDejA1v8ju1",
    "outputId": "5f0c6202-d4e1-4cca-b819-c0c86a4a9d0f"
   },
   "outputs": [],
   "source": [
    "# !pip install pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9rHPnPB6TAe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "#from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_algorithm\n",
    "from pm4py.objects.conversion.dfg import converter as dfg_conv\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.visualization.dfg import visualizer as dfg_vis_fact\n",
    "from pm4py.visualization.petrinet import visualizer as pn_vis\n",
    "\n",
    "# from torch_geometric.nn.inits import glorot, zeros \n",
    "\n",
    "#Unable to import above line, so manually copy-pasting the source code\n",
    "\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "import bisect\n",
    "import warnings\n",
    "from torch._utils import _accumulate\n",
    "from torch import randperm, default_generator\n",
    "\n",
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "        \n",
    "def random_split(dataset, lengths, generator=default_generator):\n",
    "    r\"\"\"\n",
    "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "    Optionally fix the generator for reproducible results, e.g.:\n",
    "\n",
    "    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): Dataset to be split\n",
    "        lengths (sequence): lengths of splits to be produced\n",
    "        generator (Generator): Generator used for the random permutation.\n",
    "    \"\"\"\n",
    "    if sum(lengths) != len(dataset):\n",
    "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
    "\n",
    "    indices = randperm(sum(lengths), generator=generator).tolist()\n",
    "    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rkd6Re9YYcX"
   },
   "source": [
    "# Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wbo3vcPS6ald"
   },
   "outputs": [],
   "source": [
    "# Helpdesk dataset\n",
    "\n",
    "path = 'Data/helpdesk.csv'\n",
    "save_folder = 'python_files/Results/helpdesk'\n",
    "dataset = 'helpdesk'\n",
    "num_nodes = 9\n",
    "\n",
    "# # BPI dataset\n",
    "\n",
    "# path = 'Data/bpi_12_w.csv'\n",
    "# save_folder = 'Results/bpi'\n",
    "# dataset = 'bpi'\n",
    "# num_nodes = 6 \n",
    "\n",
    "num_features = 4\n",
    "showProcessGraph = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cuda'\n",
    "num_epochs = 100\n",
    "seed_value = 42\n",
    "# lr_value = 1e-05\n",
    "\n",
    "weighted_adjacency = False\n",
    "binary_adjacency = True\n",
    "laplacian_matrix = True\n",
    "variant = 'laplacianOnBinary' # Choose from ['weighted','binary','laplacianOnWeighted','laplacianOnBinary']\n",
    "\n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlHB6xqR8VOl"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6BZ98xW4fSY"
   },
   "outputs": [],
   "source": [
    "def generate_features (df,total_activities,num_features):\n",
    "  lastcase = ''\n",
    "  firstLine = True\n",
    "  numlines = 0\n",
    "  casestarttime = None\n",
    "  lasteventtime = None\n",
    "  features = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        numlines+=1\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "    feature_list = [timediff,timediff2,timediff3,timediff4]\n",
    "    features.append(feature_list)\n",
    "\n",
    "  df['Feature Vector'] = features\n",
    "  \n",
    "  firstLine = True\n",
    "  NN_features =[]\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if firstLine:\n",
    "      features = np.zeros((total_activities,num_features))\n",
    "      features[row[1] - 1] = row[3]\n",
    "      firstLine = False\n",
    "    else:\n",
    "      if (row[3][0] == 0):\n",
    "        features = np.zeros((total_activities,num_features))\n",
    "        features[row[1] - 1] = row[3]\n",
    "      else:\n",
    "        features = np.copy(prev_row_features)\n",
    "        features[row[1] - 1] = row[3]\n",
    "    prev_row_features = features\n",
    "    NN_features.append(features)  \n",
    "  \n",
    "  return NN_features\n",
    "\n",
    "def generate_labels(df,total_activities):\n",
    "  next_activity = []\n",
    "  next_timestamp = []\n",
    "\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_activity.append(total_activities)\n",
    "      else:\n",
    "        next_activity.append(row[1]-1)\n",
    "  next_activity.append(total_activities)\n",
    "  for i,row in df.iterrows():\n",
    "    if (i != 0):\n",
    "      if (row[3][0]==0):\n",
    "        next_timestamp.append(0)\n",
    "      else:\n",
    "        next_timestamp.append(row[3][0])\n",
    "  next_timestamp.append(0)\n",
    "\n",
    "  return next_activity,next_timestamp\n",
    "\n",
    "class EventLogData(Dataset):\n",
    "  def __init__ (self, input, output):\n",
    "    self.X = input\n",
    "    self.y = output\n",
    "    self.y = self.y.to(torch.float32)\n",
    "    self.y = self.y.reshape((len(self.y),1))\n",
    "\n",
    "  #get the number of rows in the dataset\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  #get a row at a particular index in the dataset\n",
    "  def __getitem__ (self,idx):\n",
    "    return [self.X[idx],self.y[idx]]\n",
    "  \n",
    "  # get the indices for the train and test rows\n",
    "  def get_splits(self, n_test = 0.33, n_valid = 0.2):\n",
    "    train_idx,test_idx = train_test_split(list(range(len(self.X))),test_size = n_test, shuffle = False )\n",
    "    train_idx, valid_idx = train_test_split(train_idx, test_size = n_valid, shuffle = True)\n",
    "    train = Subset(self, train_idx)\n",
    "    valid = Subset(self, valid_idx)\n",
    "    test = Subset(self, test_idx)\n",
    "    return train, valid, test\n",
    "\n",
    "def prepare_data_for_Predictor(NN_features,label):\n",
    "  dataset = EventLogData(NN_features,label)\n",
    "  train, valid, test = dataset.get_splits()\n",
    "  train_dl = DataLoader(train, batch_size=1, shuffle = True)\n",
    "  valid_dl = DataLoader(valid, batch_size=1, shuffle = False)\n",
    "  test_dl = DataLoader(test, batch_size = 1, shuffle = False)\n",
    "  return train_dl, valid_dl, test_dl\n",
    "\n",
    "def generate_input_and_labels (path):\n",
    "  df = pd.read_csv(path)\n",
    "  total_unique_activities = num_nodes\n",
    "  NN_features = generate_features(df,total_unique_activities,num_features)\n",
    "  next_activity, next_timestamp = generate_labels(df,total_unique_activities)\n",
    "  NN_features = torch.Tensor(NN_features).to(torch.float32)\n",
    "  next_activity = torch.Tensor(next_activity).to(torch.float32)\n",
    "  next_timestamp = torch.Tensor(next_timestamp).to(torch.float32)\n",
    " \n",
    "  train_dl, valid_dl, test_dl = prepare_data_for_Predictor(NN_features, next_timestamp)\n",
    "  \n",
    "  return train_dl,valid_dl,test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Upqo-IbWYiCG"
   },
   "source": [
    "# Getting Adjacency Matrix from Process Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-cisiZP9_i3"
   },
   "outputs": [],
   "source": [
    "def generate_process_graph (path):\n",
    "  data = pd.read_csv(path)\n",
    "  num_nodes = data['ActivityID'].nunique() # 9 for helpdesk.csv\n",
    "  cols = ['case:concept:name','concept:name','time:timestamp']\n",
    "  data.columns = cols \n",
    "  data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "  data['concept:name'] = data['concept:name'].astype(str)\n",
    "  log = log_converter.apply(data, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "  dfg = dfg_algorithm.apply(log)\n",
    "  if showProcessGraph:\n",
    "    visualize_process_graph(dfg,log)\n",
    "  max = 0\n",
    "  min = 0\n",
    "  adj = np.zeros((num_nodes,num_nodes))\n",
    "  for k,v in dfg.items():\n",
    "    for i in range(num_nodes):\n",
    "      if(k[0] == str(i+1)):\n",
    "        for j in range(num_nodes):\n",
    "          if (k[1] == str(j+1)):\n",
    "            adj[i][j] = v\n",
    "            if (v > max): max=v\n",
    "            if (v< min): min=v\n",
    "\n",
    "  # print(\"Raw weighted adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  if binary_adjacency:\n",
    "    for i in range(num_nodes):\n",
    "      for j in range(num_nodes):\n",
    "        if (adj[i][j]!=0):\n",
    "          adj[i][j]=1\n",
    "    # print(\"Binary adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  D = np.array(np.sum(adj, axis=1))\n",
    "  D = np.matrix(np.diag(D))\n",
    "  # print(\"Degree matrix: {}\".format(D))\n",
    "  \n",
    "  adj = np.matrix(adj)\n",
    "\n",
    "  if laplacian_matrix:\n",
    "    adj = D - adj # Laplacian Transform \n",
    "    # print(\"Laplacian matrix: {}\".format(adj))\n",
    "\n",
    "  # adj = (D**-1)*adj\n",
    "  adj = fractional_matrix_power(D, -0.5)*adj*fractional_matrix_power(D, -0.5)\n",
    "  adj = torch.Tensor(adj).to(torch.float)\n",
    "  \n",
    "  # print(\"Symmetrically normalised Adjacency matrix: {}\".format(adj))\n",
    "  \n",
    "  return adj\n",
    "\n",
    "def visualize_process_graph (dfg,log):\n",
    "  dfg_gv = dfg_vis_fact.apply(dfg, log, parameters={dfg_vis_fact.Variants.FREQUENCY.value.Parameters.FORMAT: \"jpeg\"})\n",
    "  dfg_vis_fact.view(dfg_gv)\n",
    "  dfg_vis_fact.save(dfg_gv,\"dfg.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztVjtvxqYnb3"
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW3XlEZ591GP"
   },
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_features, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.in_channels = num_features\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(num_features, out_channels))\n",
    "        self.bias = Parameter(torch.Tensor(num_nodes))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = adj@x@self.weight\n",
    "        x = torch.flatten(x)\n",
    "        x = x + self.bias\n",
    "        return x\n",
    "\n",
    "class TimePredictor(torch.nn.Module):\n",
    "    def __init__(self,num_nodes, num_features = 4):\n",
    "        super(TimePredictor, self).__init__()\n",
    "\n",
    "        self.layer1 = GCNConv(num_nodes , num_features, out_channels=1)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(num_nodes,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256,1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.layer1(x,adj)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdGDSmmyYtg_"
   },
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIatIuLiFnwK"
   },
   "outputs": [],
   "source": [
    "lr_run = 0\n",
    "for lr_run in range(3):\n",
    "  if lr_run==0:\n",
    "    lr_value = 1e-03\n",
    "  elif lr_run==1:\n",
    "    lr_value = 1e-04\n",
    "  elif lr_run==2:\n",
    "    lr_value = 1e-05\n",
    "  run = 0\n",
    "  for run in range(num_runs):\n",
    "    print(\"Run: {}, Learning Rate: {}\".format(run+1,lr_value))\n",
    "    model = TimePredictor(num_nodes, num_features)  \n",
    "    train_dl, valid_dl, test_dl = generate_input_and_labels(path)\n",
    "    adj = generate_process_graph(path)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr_value)\n",
    "\n",
    "    # print(\"************* Timestamp Predictor ***************\")\n",
    "    # print(\"Train size: {}, Validation size:{}, Test size: {}\".format(len(train_dl.dataset),len(valid_dl.dataset),len(test_dl.dataset)))\n",
    "    # print(model)\n",
    "    model = model.to(device)\n",
    "    adj = adj.to(device)\n",
    "    epochs_plt = []\n",
    "    mae_plt = []\n",
    "    valid_loss_plt = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        predictions, actuals = list(),list()\n",
    "        num_train = 0\n",
    "\n",
    "        for i, (inputs,targets) in enumerate(train_dl):\n",
    "\n",
    "          inputs,targets = inputs.to(device), targets.to(device)\n",
    "          optimizer.zero_grad() # Clearing the gradients\n",
    "\n",
    "          yhat = model(inputs[0],adj)\n",
    "          loss = criterion(yhat.reshape((1,-1)),targets[0].to(torch.long).reshape((1,-1)))\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          training_loss+= loss.item()\n",
    "          num_train+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "          model.eval()\n",
    "          num_valid = 0\n",
    "          validation_loss = 0\n",
    "          for i,(inputs,targets) in enumerate(valid_dl):\n",
    "            inputs,targets = inputs.to(device),targets.to(device)\n",
    "            yhat_valid = model(inputs[0],adj)\n",
    "            loss_valid = criterion(yhat_valid.reshape((1,-1)),targets[0].to(torch.long).reshape((1,-1)))\n",
    "            validation_loss+= loss_valid.item()\n",
    "            num_valid+= 1\n",
    "\n",
    "        avg_training_loss = training_loss/num_train\n",
    "        avg_training_loss = avg_training_loss/86400\n",
    "        avg_validation_loss = validation_loss/num_valid\n",
    "        avg_validation_loss = avg_validation_loss/86400\n",
    "\n",
    "        if (epoch==0): \n",
    "          best_loss = avg_validation_loss\n",
    "          torch.save(model.state_dict(),'TimestampPredictor_parameters.pt')\n",
    "        \n",
    "        if (avg_validation_loss < best_loss):\n",
    "          torch.save(model.state_dict(),'TimestampPredictor_parameters.pt')\n",
    "          best_loss = avg_validation_loss\n",
    "          \n",
    "        print(\"Epoch: {}, Training MAE : {}, Validation loss : {}\".format(epoch,avg_training_loss,avg_validation_loss))\n",
    "        epochs_plt.append(epoch+1)\n",
    "        mae_plt.append(avg_training_loss)\n",
    "        valid_loss_plt.append(avg_validation_loss)\n",
    "\n",
    "    model.load_state_dict(torch.load('TimestampPredictor_parameters.pt'))\n",
    "    torch.save(model.state_dict(),'{}/TimestampPredictor_parameters_{}_{}_{}_run{}.pt'.format(save_folder,dataset,variant,lr_value,run))\n",
    "    filepath = '{}/Loss_{}_{}_{}_run{}.txt'.format(save_folder,dataset,variant,lr_value,run)\n",
    "\n",
    "    with open(filepath, 'w') as file:\n",
    "        for item in zip(epochs_plt,mae_plt,valid_loss_plt):\n",
    "            file.write(\"{}\\n\".format(item))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOPS/LnFBo+T70fVUPztuEf",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1dGjxvgWOx7LkyMw3aJTimuqvfY0xLWGn",
   "name": "Copy of GCN_TimePredictor_(Training).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
